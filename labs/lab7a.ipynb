{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Ag0r1CeJ-7X"
   },
   "source": [
    "# [CSCI 3397/PSYC 3317] Lab 7a: MLP and CNN with Pytorch\n",
    "\n",
    "**Posted:** Monday, March 14, 2022\n",
    "\n",
    "**Due:** Friday, March 18, 2022\n",
    "\n",
    "__Total Points__: 3 pts\n",
    "\n",
    "__Submission__: please rename the .ipynb file as __\\<your_username\\>\\_lab7a.ipynb__ before you submit it to canvas. Example: weidf_lab7a.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7kHfyrM6gaC"
   },
   "source": [
    "# 1.  MLP with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kJwrM8O-1Ps"
   },
   "source": [
    "To build a deep learning model in Pytorch, we need to \n",
    "- define the needed layers under `__init__()` \n",
    "- specify the model computation under `foward()`. \n",
    "\n",
    "\n",
    "The gradient computation is automatically done under the parent's `backward()` (can be overwritten if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNnhmOmS8VCD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_oneHiddenLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_neuron, nonlinear=F.relu):\n",
    "        super(MLP_oneHiddenLayer, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, num_neuron)\n",
    "        self.fc2 = nn.Linear(num_neuron, output_dim)\n",
    "        self.nonlinear = nonlinear    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7pg8H0_AmDu"
   },
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "\n",
    "num_input, num_output, num_neuron = 10,20,1 \n",
    "model = MLP_oneHiddenLayer(num_input, num_output, num_neuron)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4L6KqMOK-sts"
   },
   "source": [
    "## [1 pt] Exercise 1. N-layer MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-ZqgCXsZF0b"
   },
   "source": [
    "**[TODO]** Fill in the missing code.\n",
    "\n",
    "Let's build a MLP model with the specified number of hidden layers and number of neurons.\n",
    "\n",
    "**Course material: Lecture 12, page 31-32**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1O5rw1N6gkH"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_neuron=[], nonlinear=F.relu):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        if len(num_neuron) == 0:\n",
    "            layers += [nn.Linear(input_dim, output_dim)]\n",
    "        else:\n",
    "            # first layer\n",
    "            layers += [nn.Linear(input_dim, num_neuron[0]), nn.ReLU()]\n",
    "            \n",
    "            #### YOUR CODE STARTS HERE ####\n",
    "\n",
    "            #### YOUR CODE ENDS HERE ####\n",
    "            \n",
    "            # last layer\n",
    "            layers += [nn.Linear(num_neuron[-1], output_dim)]\n",
    "\n",
    "        # pytorch syntax to create layers from a list\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "   \n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch    x = x.view(-1, 32*32*3)\n",
    "        x = self.layers(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# test case \n",
    "num_input, num_output = 10,20\n",
    "num_neuron = [128, 128, 128]\n",
    "model_mlp = MLP(num_input, num_output, num_neuron)\n",
    "\n",
    "print(model_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0uS4bq9WZoN"
   },
   "source": [
    "# 2. CNN with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmVPsGp0WcPE"
   },
   "source": [
    "Below is the detailed architecture details of the famous AlexNet. Let's go through the details of its layers by computing the input/output size. \n",
    "<img height=400 src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-19-16-01-03.png\"/>\n",
    "<img height=400 src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2021/03/Screenshot-from-2021-03-19-16-01-13.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deMROhFbZ5YJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# this one is slightly different from the orignal AlexNet paper\n",
    "alexnet = models.alexnet()\n",
    "alexnet.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD_l6o8VWlxo"
   },
   "source": [
    "## (a) Conv Layer.\n",
    "Below is the function to compute the output size of a convolutional layer.\n",
    "\n",
    "**Course Material: Lecture 13, page 50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1647142841196,
     "user": {
      "displayName": "Donglai Wei",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghp_r_BM-hcnzseHQOqKEog51cEpxJX-9EaMyIO=s64",
      "userId": "05000800795689376079"
     },
     "user_tz": 300
    },
    "id": "dJ-sNJfwW_7S",
    "outputId": "c6c6adcb-abab-4337-e247-cb96ff607b9d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSizeConv(input_size, kernel_size, pad_size, stride_size):  \n",
    "    # input_size: N x C_in x H x W\n",
    "    # kernel_size: C_out x C_in x KH x KW\n",
    "    # stride: [Sx, Sy]\n",
    "    # pad: [Px, Py]\n",
    "    # -----\n",
    "    # output_size: N x C_out x OH x OW\n",
    "    output_size = np.zeros(4)\n",
    "    # 0: batch_size\n",
    "    output_size[0] = input_size[0]\n",
    "    # 1: channel size\n",
    "    output_size[1] = kernel_size[0]\n",
    "    # 2/3: spatial dimension (height/weight)\n",
    "    output_size[2] = (input_size[2] + pad_size[0]*2 - kernel_size[2]) // stride_size[0] + 1\n",
    "    output_size[3] = (input_size[3] + pad_size[1]*2 - kernel_size[3]) // stride_size[1] + 1\n",
    "\n",
    "    return output_size.astype(int)\n",
    "\n",
    "\n",
    "## test case\n",
    "input_size = [10,3,227,227]\n",
    "kernel_size = [96,3,11,11]\n",
    "pad_size = [0,0]\n",
    "stride_size = [4,4]\n",
    "\n",
    "output_gt = [10,96,55,55]\n",
    "output_pred = getSizeConv(input_size, kernel_size, pad_size, stride_size)\n",
    "\n",
    "print('gt: ', output_gt)\n",
    "print('pred: ', output_pred)\n",
    "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEWlPrSTaF1x"
   },
   "source": [
    "## (b) ReLU layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctmR3r5OaIzD"
   },
   "outputs": [],
   "source": [
    "def my_relu(input_tensor): \n",
    "    output_tensor = input_tensor.copy()\n",
    "    output_tensor[output_tensor < 0] = 0\n",
    "    return output_tensor\n",
    "\n",
    "test_tensor = torch.randn([2,64,55,55])\n",
    "relu1 = alexnet._modules['features'][1]\n",
    "\n",
    "outReLU_my = my_relu(test_tensor.detach().numpy())\n",
    "outReLU_pt = relu1(test_tensor).detach().numpy()\n",
    "\n",
    "np.abs(outReLU_my - outReLU_pt).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lcgFIHDXUDn"
   },
   "source": [
    "## (c) Pooling layer\n",
    "Below is the function to compute the output size of a pooling layer.\n",
    "\n",
    "**Course Material: Lecture 13, page 68**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqU5gInaXhVe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSizePool(input_size, kernel_size, pad_size, stride_size):  \n",
    "    # input_size: N x C_in x H x W\n",
    "    # kernel_size: KH x KW\n",
    "    # pad_size: [Px, Py]\n",
    "    # stride_size: [Sx, Sy]  \n",
    "    # -----\n",
    "    # output_size: N x C_in x OH x OW\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = [kernel_size, kernel_size]\n",
    "    if isinstance(stride_size, int):\n",
    "        stride_size = [stride_size, stride_size]\n",
    "    if isinstance(pad_size, int):\n",
    "        pad_size = [pad_size, pad_size]    \n",
    "\n",
    "    output_size = np.zeros(4)\n",
    "    # 0: batch_size\n",
    "    output_size[0] = input_size[0]\n",
    "    # 1: channel size\n",
    "    output_size[1] = input_size[1]\n",
    "    # 2/3: spatial dimension (height/weight)\n",
    "    output_size[2] = (input_size[2] + pad_size[0]*2 - kernel_size[0]) // stride_size[0] + 1\n",
    "    output_size[3] = (input_size[3] + pad_size[1]*2 - kernel_size[1]) // stride_size[1] + 1\n",
    "    return output_size.astype(int)\n",
    "\n",
    "\n",
    "## test case\n",
    "input_size = [10,96,55,55]\n",
    "kernel_size = [3,3]\n",
    "stride_size = [2,2]\n",
    "pad_size = [0,0]\n",
    "output_gt = [10,96,27,27]\n",
    "output_pred = getSizePool(input_size, kernel_size, pad_size, stride_size)\n",
    "\n",
    "print('gt: ', output_gt)\n",
    "print('pred: ', output_pred)\n",
    "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfGTemfMahL9"
   },
   "source": [
    "Function to implement the max-pool layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JbJdod3aP-K"
   },
   "outputs": [],
   "source": [
    "def my_pool(pool_ops, input_tensor, kernel_size, pad_size, stride_size):\n",
    "    # input_tensor size: N x C_in x H x W\n",
    "    # kernel_size: KH x KW\n",
    "    # pad_size: [Px, Py]\n",
    "    # stride_size: [Sx, Sy]\n",
    "    # output_size: N x C_in x OH x OW\n",
    "    output_size = getSizePool(input_tensor.shape, kernel_size, pad_size, stride_size)\n",
    "    output_tensor = np.zeros(output_size)\n",
    "\n",
    "    if isinstance(kernel_size, int):\n",
    "        kernel_size = [kernel_size, kernel_size]\n",
    "    if isinstance(stride_size, int):\n",
    "        stride_size = [stride_size, stride_size]\n",
    "    if isinstance(pad_size, int):\n",
    "        pad_size = [pad_size, pad_size]    \n",
    "\n",
    "    for x in range(output_size[2]):\n",
    "        for y in range(output_size[3]):\n",
    "            patch = input_tensor[:, :, x*stride_size[0]:x*stride_size[0]+kernel_size[0],\\\n",
    "                  y*stride_size[1]:y*stride_size[1]+kernel_size[1]]\n",
    "            if pool_ops == 'max':\n",
    "                output_tensor[:,:,x,y] = patch.max(axis=2).max(axis=2)\n",
    "            elif pool_ops == 'avg':\n",
    "                output_tensor[:,:,x,y] = patch.mean(axis=2).mean(axis=2)\n",
    "    return output_tensor\n",
    "\n",
    "test_tensor = torch.randn([2,64,55,55])\n",
    "pool1 = alexnet._modules['features'][2]\n",
    "\n",
    "outPool_my = my_pool('max', test_tensor.detach().numpy(), pool1.kernel_size, pool1.padding, pool1.stride)\n",
    "outPool_pt = pool1(test_tensor).detach().numpy()\n",
    "\n",
    "print('Max abs diff for maxpool', np.abs(outPool_my - outPool_pt).max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6JDCwSuZO3r"
   },
   "source": [
    "## (d) Reshape Layer.\n",
    "Below is the function to compute the output size of a reshape layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_buP9OMZQDq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSizeReshape(input_size):\n",
    "    # input_size: N x ... (multi-dim)\n",
    "    # -----\n",
    "    # output_size: N x O\n",
    "    output_size = np.array([input_size[0], np.prod(input_size[1:])])\n",
    "    return output_size\n",
    "\n",
    "\n",
    "## test case\n",
    "input_size = [10, 256, 6, 6]\n",
    "output_gt = [10, 9216]\n",
    "output_pred = getSizeReshape(input_size)\n",
    "\n",
    "print('gt: ', output_gt)\n",
    "print('pred: ', output_pred)\n",
    "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWKPJqXLamdo"
   },
   "source": [
    "## (e) Dropout layer\n",
    "\n",
    "**Course materials: Lecture 13, page 72-74**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzpg1c8rao6a"
   },
   "outputs": [],
   "source": [
    "def my_drop(input_tensor, p):\n",
    "    output_tensor = input_tensor.copy()\n",
    "    rand = np.random.rand(output_tensor.shape[0], output_tensor.shape[1])\n",
    "    output_tensor[rand<p] = 0  \n",
    "    return output_tensor\n",
    "\n",
    "test_tensor = torch.randn([100,4096])\n",
    "drop1 = alexnet._modules['classifier'][0]\n",
    "\n",
    "outDrop_my = my_drop(test_tensor.detach().numpy(), drop1.p)\n",
    "outDrop_pt = drop1(test_tensor).detach().numpy()\n",
    "\n",
    "num = float(test_tensor.nelement())\n",
    "# doesn't have to be 0\n",
    "np.abs((outDrop_my==0).sum()/num - (outDrop_pt==0).sum()/num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxsXQU_GXl_C"
   },
   "source": [
    "## (f) **Exercise 2** [2 pts] FC layer.\n",
    "Implement the function to compute the output size of a fully-connected (fc) layer.\n",
    "\n",
    "**Course Material: Lecture 12, page 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqC9EbyZZNah"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getSizeFc(input_size, weight_size):  \n",
    "    # input_size: N x L\n",
    "    # weight_size: M x L\n",
    "    # -----\n",
    "    # output_size: N x M\n",
    "    \n",
    "    #### YOUR CODE STARTS HERE ####\n",
    "\n",
    "    #### YOUR CODE ENDS HERE ####    \n",
    "    \n",
    "    return output_size\n",
    "\n",
    "\n",
    "## test case\n",
    "input_size = [10, 4096]\n",
    "weight_size = [1000, 4096]\n",
    "output_gt = [10,1000]\n",
    "output_pred = getSizeFc(input_size, weight_size)\n",
    "\n",
    "print('gt: ', output_gt)\n",
    "print('pred: ', output_pred)\n",
    "print('max abs diff: ', np.abs(output_pred-output_gt).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQYTeyXbRzd"
   },
   "source": [
    "After you finished the exercise above, below is the function to implement the FC layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_IlTxvQa8CZ"
   },
   "outputs": [],
   "source": [
    "def my_fc(input_tensor, weight_tensor, bias_tensor):\n",
    "    # input_tensor size: N x L\n",
    "    # kernel_size: M x L  \n",
    "    # output_size: N x M  \n",
    "\n",
    "    output_size = getSizeFc(input_tensor.shape, weight_tensor.shape)\n",
    "    output_tensor = np.zeros(output_size)\n",
    "    for sample_id in range(input_tensor.shape[0]):\n",
    "        # hint: numpy matrix multiplication is np.matmul\n",
    "        # a*b is the element-wise multiplication\n",
    "        #### YOUR CODE STARTS HERE ####\n",
    "\n",
    "        #### YOUR CODE ENDS HERE ####        \n",
    "    return output_tensor\n",
    "\n",
    "test_tensor = torch.randn([2,4096])\n",
    "fc7 = alexnet._modules['classifier'][4]\n",
    "\n",
    "outFc_my = my_fc(test_tensor.detach().numpy(), fc7.weight.detach().numpy(), fc7.bias.detach().numpy())\n",
    "outFc_pt = fc7(test_tensor).detach().numpy()\n",
    "\n",
    "np.abs(outFc_my - outFc_pt).max()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQIIHCjpZRne"
   },
   "source": [
    "## (e) Putting things together: here's the output size for each layer of AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3vNkdj5ZRyc"
   },
   "outputs": [],
   "source": [
    "alexNet = [\\\n",
    "    ['conv1',[96,3,11,11],[0,0],[4,4]],\\\n",
    "    ['pool1',[3,3],[0,0],[2,2]],\\\n",
    "    ['conv2',[256,96,5,5],[2,2],[1,1]],\\\n",
    "    ['pool2',[3,3],[0,0],[2,2]],\\\n",
    "    ['conv3',[384,256,3,3],[1,1],[1,1]],\\\n",
    "    ['conv4',[384,384,3,3],[1,1],[1,1]],\\\n",
    "    ['conv5',[256,384,3,3],[1,1],[1,1]],\\\n",
    "    ['pool5',[3,3],[0,0],[2,2]],\\\n",
    "    ['reshape'],\\\n",
    "    ['fc6',[4096,9216]],\\\n",
    "    ['fc7',[4096,4096]],\\\n",
    "    ['fc8',[1000,4096]]\n",
    "    ]\n",
    "\n",
    "tensor_size = [10,3,227,227]\n",
    "tensor_size = [10,3,600,800]\n",
    "for layer in alexNet:\n",
    "    if 'conv' in layer[0]:\n",
    "        layer_name, kernel_size, pad_size, stride_size = layer\n",
    "        tensor_size = getSizeConv(tensor_size, kernel_size, pad_size, stride_size)\n",
    "        print(layer_name, tensor_size)\n",
    "    elif 'pool' in layer[0]:\n",
    "        layer_name, kernel_size, pad_size, stride_size = layer\n",
    "        tensor_size = getSizePool(tensor_size, kernel_size, pad_size, stride_size)\n",
    "        print(layer_name, tensor_size)    \n",
    "    elif 'reshape' in layer[0]:    \n",
    "        tensor_size = getSizeReshape(tensor_size)\n",
    "        print(layer[0], tensor_size)\n",
    "    elif 'fc' in layer[0]:\n",
    "        layer_name, weight_size = layer\n",
    "        tensor_size = getSizeFc(tensor_size, weight_size)\n",
    "    print(layer_name, tensor_size)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Q6JDCwSuZO3r",
    "jWKPJqXLamdo"
   ],
   "name": "lab7b_sol.ipynb",
   "provenance": [
    {
     "file_id": "1hR-DQvve8uEX2zH8h4y1XgP1atKRUl0g",
     "timestamp": 1633281385748
    },
    {
     "file_id": "15ec--mRyf2Trd2zwdqWqLM_Qc4JX1iNw",
     "timestamp": 1551367509167
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
